# Different-Ways-to-Fit-Hyperplane-to-the-Data

![image](https://github.com/ramangarg007/Different-Ways-to-Fit-Hyperplane-to-the-Data/assets/15136599/eb411883-dc7e-406a-9073-dcf62466da71)
![image](https://github.com/ramangarg007/Different-Ways-to-Fit-Hyperplane-to-the-Data/assets/15136599/f17d59ae-2d01-4ae1-ba27-2d4324ec5a99)
![image](https://github.com/ramangarg007/Different-Ways-to-Fit-Hyperplane-to-the-Data/assets/15136599/47bd32dd-190c-4e2e-9553-440ce83d8b79)


## Introduction:
In the realm of machine learning and statistical modeling, the techniques of Linear Regression, Logistic Regression, and Support Vector Machine (SVM) play pivotal roles in various applications. These methods are widely employed for their ability to model relationships between variables and classify data points into distinct classes. At the core of their functionality lies the concept of fitting hyperplanes to the data, facilitating effective separation and prediction tasks.

The motivation behind writing this notebook is to understand how there three very different algorithms(in the vanilla form) are basically just separated by different loss functions.

NOTE: these three algorithms are not used for similar tasks and are employed for different use cases.

## Project Objective: The objective of this Jupyter notebook is to elucidate how three seemingly distinct algorithms (linear regression, logistic regression, and SVM) in their vanilla forms are fundamentally united by the common thread of different loss functions governing their behavior.
For each algorithm we will see the loss function, the derivatives and the implementation in python.

1. Linear Regression
2. Logistic Regression
3. Support Vector Machine
